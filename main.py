import json
import time
import itertools
import numpy as np
import pandas as pd
import torch
import argparse  
import logging
import sys
from model import get_model
from dataset import get_dataset
from utils.seed_utils import set_seed
from config import nan_inf_clip_factor
import os
# transforms
from transforms import *

# æœç´¢ç©ºé—´
from utils.param_utils import get_params_space_and_org

# ========= åŸºæœ¬é…ç½® =========
parser = argparse.ArgumentParser(description="æŒ‡å®šæ—¶åºæ¨¡å‹è¿è¡Œçš„æ˜¾å¡ç¼–å·")
parser.add_argument("--gpu", type=int, default=0, help="æ˜¾å¡ç¼–å·ï¼ˆé»˜è®¤0ï¼Œå¤šæ˜¾å¡æ—¶å¯æŒ‡å®š1ã€2ç­‰ï¼‰")
parser.add_argument("--data", type=str, required=True, 
                    choices=["ETTh1","ETTh2","ETTm1","ETTm2","Exchange","Weather","Electricity","Traffic"],
                    help="æ•°æ®é›†åç§°ï¼ˆå¿…å¡«ï¼Œå¯é€‰å€¼ï¼šETTh1/ETTh2/ETTm1/ETTm2/Exchange/Weather/Electricity/Trafficï¼‰")
args = parser.parse_args()

# ä¿®æ”¹ï¼šæ ¹æ®å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šDEVICEï¼Œè€Œéå›ºå®šcuda:0 ğŸ‘ˆ
DEVICE =  f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu"
SEED = 42
print(f"ç¨‹åºå°†ä½¿ç”¨çš„è®¾å¤‡: {DEVICE}") 
MODEL_NAME = "TimerXL"
DATA_NAME  = args.data    # å¯é€‰ï¼šETTh1,ETTh2,ETTm1,ETTm2,Exchange,Weather,Electricity,Traffic
EVAL_BATCH_SIZE = 16

TARGET_COLUMN = "OT"
MAX_SEQ_LEN   = 96 * 7     # éœ€æ»¡è¶³: MAX_SEQ_LEN <= train_len/2         # ! è¾“å…¥é•¿åº¦1440
PRED_LEN      = 96

# æœç´¢æ§åˆ¶
PRIMARY_METRIC = "mse"
MAX_TRIALS = None                # None=å…¨ç©ºé—´ï¼›å¦åˆ™éšæœºæŠ½æ ·
RESULT_CSV = f"dp_search_results{args.data}_{args.gpu}.csv"

# ========= æŒ‡æ ‡ =========
def mse(y_true, y_pred):
    return float(np.mean((y_true - y_pred) ** 2))

def mae(y_true, y_pred):
    return float(np.mean(np.abs(y_true - y_pred)))

def smape(y_true, y_pred, eps=1e-8):
    denom = (np.abs(y_true) + np.abs(y_pred) + eps)
    return float(200.0 * np.mean(np.abs(y_pred - y_true) / denom))

# ========= æ•°æ®æ‰¹ç”Ÿæˆ=========
def get_eval_batches(dataset, mode="test", batch_size=32,
                     target=TARGET_COLUMN, max_seq_len=MAX_SEQ_LEN, pred_len=PRED_LEN):
    assert mode in ["train", "val", "test"]
    idx_list = dataset.get_available_idx_list(mode, max_seq_len, pred_len)
    N = len(idx_list)
    for s in range(0, N, batch_size):
        e = min(N, s + batch_size)
        batch_idx = idx_list[s:e]
        Xb = np.zeros((len(batch_idx), max_seq_len, 1), dtype=np.float32)
        Yb = np.zeros((len(batch_idx), pred_len,    1), dtype=np.float32)
        for i, real_idx in enumerate(batch_idx):
            hwl = dataset.get_history_with_label(
                target=target, flag=mode, real_idx=real_idx,
                max_seq_len=max_seq_len, pred_len=pred_len
            )
            hist = hwl[:max_seq_len].reshape(-1, 1)
            lab  = hwl[max_seq_len:].reshape(-1, 1)
            Xb[i] = hist
            Yb[i] = lab
        yield Xb, Yb

# ========= æ¨¡å‹æ¨ç†=========
@torch.no_grad()
def model_infer(model, x_np):
    x = torch.from_numpy(x_np).float().to(DEVICE)
    y_pred = model.forcast(x, PRED_LEN)
    return y_pred

# ========= æ„å»ºå¤„ç†å™¨=========
def build_pipeline_fns(cfg, dataset, input_data, history_data):
    """
    è¿”å› pre_fn, post_fn ä¸¤ä¸ªå‡½æ•°ã€‚
    åªå¯¹ cfg æŒ‡å®šçš„ç®—å­å®ä¾‹åŒ–å¹¶æŒ‰å›ºå®šæ¬¡åºä¸²æ¥ï¼š
      Pre é¡ºåºï¼šSampler -> Trimmer -> Aligner -> Inputer -> Denoiser -> Warper -> Normalizer -> Differentiator
      Posté¡ºåºï¼š               Differentiator -> Normalizer -> Warper -> (å…¶ä½™ä¸ºæ’ç­‰ï¼Œä¸” Sampler ä¸åš post)
    æ³¨ï¼š
      - Sampler çš„ post_process åœ¨ä½ å®ç°é‡Œä¼šæ”¹å˜é•¿åº¦ï¼Œä¸é€‚åˆä½œä¸ºé¢„æµ‹åè¿˜åŸï¼Œè¿™é‡Œæ˜ç¡®**ä¸è°ƒç”¨**ã€‚
      - Trimmer.post_process åªä¼šè£åˆ° pred_lenï¼›è‹¥æ¨¡å‹å·²è¾“å‡º pred_lenï¼Œç­‰ä»·æ’ç­‰ã€‚
      - Aligner/Inputer/Denoiser post å‡ä¸ºæ’ç­‰ï¼Œæ•…æ— éœ€çº³å…¥ post_fnã€‚
      - Normalizer éœ€è¦ dataset_scalerï¼ˆå½“ mode='dataset' æ—¶ï¼‰â†’ ç”¨ train æ®µæ‹Ÿåˆæ›´åˆç†ã€‚
    """
    # â€”â€” clip å› å­ï¼ˆä¼ ç»™ä¼šç”¨åˆ°çš„ç®—å­ï¼‰â€”â€”
    clip_factor = cfg.get("clip_factor", "none")

    # â€”â€” Samplerï¼ˆå¯é€‰ï¼‰â€”â€”
    sampler = None
    if int(cfg.get("sampler_factor", 1)) != 1:
        sampler = Sampler(factor=int(cfg["sampler_factor"]))

    # â€”â€” Trimmerï¼ˆå¯é€‰ï¼‰â€”â€”
    trimmer = None
    tr_seq = int(cfg.get("trimmer_seq_len", MAX_SEQ_LEN))
    if tr_seq != MAX_SEQ_LEN:
        trimmer = Trimmer(seq_l=tr_seq, pred_l=PRED_LEN)

    # â€”â€” Alignerï¼ˆå¯é€‰ï¼‰â€”â€”
    aligner = None
    if cfg.get("aligner_mode", "none") != "none" and cfg.get("aligner_method", "none") != "none":
        # è¿™é‡Œ data_patch_len/model_patch_len ç”¨è¾“å…¥é•¿åº¦å¯¹é½ï¼›è‹¥ä½ çš„æ¨¡å‹æœ‰ç‰¹å®š patch é•¿åº¦ï¼Œå¯æ›¿æ¢
        aligner = Aligner(mode=cfg["aligner_mode"],
                          method=cfg["aligner_method"],
                          data_patch_len=MAX_SEQ_LEN,
                          model_patch_len=MAX_SEQ_LEN)

    # â€”â€” Inputerï¼ˆå¯é€‰ï¼‰â€”â€”
    inputer = None
    if cfg.get("inputer_detect_method", "none") != "none" and cfg.get("inputer_fill_method", "none") != "none":
        inputer = Inputer(detect_method=cfg["inputer_detect_method"],
                          fill_method=cfg["inputer_fill_method"],
                          history_seq=history_data)

    # â€”â€” Denoiserï¼ˆå¯é€‰ï¼‰â€”â€”
    denoiser = None
    if cfg.get("denoiser_method", "none") != "none":
        denoiser = Denoiser(method=cfg["denoiser_method"])

    # â€”â€” Warperï¼ˆå¯é€‰ï¼Œå¯é€†ï¼‰â€”â€”
    warper = None
    if cfg.get("warper_method", "none") != "none":
        warper = Warper(method=cfg["warper_method"], clip_factor=clip_factor)

    # â€”â€” Normalizerï¼ˆå¯é€‰ï¼Œå¯é€†ï¼‰â€”â€”
    normalizer = None
    if cfg.get("normalizer_method", "none") != "none" and cfg.get("normalizer_mode", "none") != "none":
        mode = cfg["normalizer_mode"]
        dataset_scaler = None
        if mode == "dataset":
            # ç”¨ train æ®µæ‹Ÿåˆæ›´åˆç†
            dataset_scaler = dataset.get_scaler(method=cfg["normalizer_method"], target=TARGET_COLUMN)
        normalizer = Normalizer(
            method=cfg["normalizer_method"], mode=mode,
            input_data=input_data, history_data=history_data,
            dataset_scaler=dataset_scaler,
            ratio=cfg.get("normalizer_ratio", 1),
            clip_factor=clip_factor
        )

    # â€”â€” Differentiatorï¼ˆå¯é€†ï¼‰â€”â€”
    differentiator = None
    if int(cfg.get("differentiator_n", 0)) > 0:
        differentiator = Differentiator(n=int(cfg["differentiator_n"]), clip_factor=clip_factor)

    # # â€”â€” Decomposerï¼ˆå¦‚éœ€å¯ç”¨ï¼Œæ³¨æ„å…¶å¼€é”€/å¯é€†æ€§ï¼‰â€”â€”
    # decomposer = None
    # if cfg.get("decomposer_components","none")!="none" and cfg.get("decomposer_period","none")!="none":
    #     decomposer = Decomposer(period=int(cfg["decomposer_period"]),
    #                             component_for_model=cfg["decomposer_components"])

    def pre_fn(x):
        if sampler is not None:       x = sampler.pre_process(x)
        if trimmer is not None:       x = trimmer.pre_process(x)
        if aligner is not None:       x = aligner.pre_process(x)
        if inputer is not None:       x = inputer.pre_process(x)
        if denoiser is not None:      x = denoiser.pre_process(x)
        if warper is not None:        x = warper.pre_process(x)
        if normalizer is not None:    x = normalizer.pre_process(x)
        if differentiator is not None:x = differentiator.pre_process(x)
        # if decomposer is not None: x = decomposer.pre_process(x)
        return x

    def post_fn(y):
        # åªåšâ€œæœ‰æ„ä¹‰çš„åå˜æ¢â€ï¼Œä¸”ä¸¥æ ¼é€†åº
        if differentiator is not None: y = differentiator.post_process(y)
        if normalizer   is not None:   y = normalizer.post_process(y)
        if warper       is not None:   y = warper.post_process(y)
        # denoiser/inputer/aligner éƒ½æ˜¯æ’ç­‰ postï¼›sampler çš„ post ä¼šæ”¹å˜é•¿åº¦ï¼Œè¿™é‡Œ**æœ‰æ„ä¸è°ƒç”¨**
        if trimmer      is not None:   y = trimmer.post_process(y)  # è£æˆ pred_lenï¼ˆè‹¥å·²æ˜¯åˆ™ä¸æ”¹ï¼‰
        # if decomposer is not None:   y = decomposer.post_process(y)
        return y

    return pre_fn, post_fn


# ========= ä» param_utils ç”Ÿæˆæœç´¢ç»„åˆ =========
def get_search_space():
    params_space, origin = get_params_space_and_org()
    # æœ¬è„šæœ¬å·²æ¥çº¿çš„é”®ï¼š
    used_keys = [
        "sampler_factor",
        "trimmer_seq_len",
        "aligner_mode", "aligner_method",
        "inputer_detect_method", "inputer_fill_method",
        "denoiser_method",
        "warper_method",
        "normalizer_method", "normalizer_mode", "normalizer_ratio",
        "differentiator_n",
        "clip_factor",
        # "decomposer_period","decomposer_components",  # å¦‚éœ€å¯ç”¨è‡ªè¡ŒåŠ å…¥
    ]
    space = {}
    for k in used_keys:
        if k in params_space:
            space[k] = list(params_space[k]["values"])
        else:
            # ç”¨ origin å…œåº•ä¸ºå•å€¼
            space[k] = [origin.get(k)]
    baseline = {k: origin.get(k) for k in used_keys}
    return used_keys, space, baseline


def enumerate_combinations(space, max_trials=None, seed=SEED, ensure_include=None):
    keys = list(space.keys())
    grid = list(itertools.product(*[space[k] for k in keys]))
    combos = [{k: v for k, v in zip(keys, tpl)} for tpl in grid]
    if ensure_include is not None and ensure_include not in combos:
        combos.insert(0, ensure_include)
    if max_trials is None or max_trials >= len(combos):
        return combos
    rng = np.random.default_rng(seed)
    picked = set(rng.choice(len(combos), size=max_trials, replace=False).tolist())
    if ensure_include is not None:
        try:
            base_idx = combos.index(ensure_include)
            if base_idx not in picked:
                picked.pop()
                picked.add(base_idx)
        except ValueError:
            pass
    return [combos[i] for i in sorted(picked)]

# ========= è¯„æµ‹ä¸€ä¸ªç»„åˆ =========
@torch.no_grad()
def evaluate_combo(model, dataset, cfg, split, batch_size=EVAL_BATCH_SIZE):
    mse_list, mae_list, smape_list = [], [], []

    for x_np, y_np in get_eval_batches(dataset, mode=split, batch_size=batch_size,
                                       target=TARGET_COLUMN, max_seq_len=MAX_SEQ_LEN, pred_len=PRED_LEN):
        # ğŸ‘‡ æ¯ä¸ª batch éƒ½ç”¨å®ƒè‡ªå·±çš„æ•°æ®æ„å»ºå¤„ç†å™¨ï¼ˆå…³é”®æ”¹åŠ¨ï¼‰
        pre_fn, post_fn = build_pipeline_fns(
            cfg, dataset,
            input_data=x_np,    # for normalizer(mode='input')
            history_data=x_np   # for normalizer(mode='history')
        )

        x_proc = pre_fn(x_np.copy())
        y_pred = model_infer(model, x_proc)
        y_rest = post_fn(y_pred)

        mse_list.append(mse(y_np, y_rest))
        mae_list.append(mae(y_np, y_rest))
        smape_list.append(smape(y_np, y_rest))

    return {
        "mse": float(np.mean(mse_list)),
        "mae": float(np.mean(mae_list)),
        "smape": float(np.mean(smape_list)),
        "count_batches": len(mse_list),
    }
    
# ========= ä¸»æµç¨‹ï¼šåœ¨éªŒè¯é›†ä¸Šæœç´¢æœ€ä¼˜ï¼Œå†åˆ°æµ‹è¯•é›†å¯¹æ¯” =========
def main():
    log_format = "%(asctime)s - %(levelname)s - %(module)s - %(message)s"
    # é…ç½®æ—¥å¿—çº§åˆ«ï¼ˆDEBUG: æœ€è¯¦ç»†ï¼ŒINFO: å¸¸è§„ä¿¡æ¯ï¼ŒWARNING: è­¦å‘Šï¼ŒERROR: é”™è¯¯ï¼‰
    logging.basicConfig(
        level=logging.INFO,  # æ•è·INFOåŠä»¥ä¸Šçº§åˆ«æ—¥å¿—
        format=log_format,
        # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶ï¼ˆä½†æ–‡ä»¶è¾“å‡ºä¼šè¢«bashè„šæœ¬é‡å®šå‘è¦†ç›–ï¼Œè¿™é‡Œä¸»è¦ç¡®ä¿æ§åˆ¶å°è¾“å‡ºè¢«æ•è·ï¼‰
        handlers=[
            logging.StreamHandler(sys.stdout),  # è¾“å‡ºåˆ°stdoutï¼ˆä¼šè¢«bashçš„>é‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶ï¼‰
            # è‹¥éœ€è¦é¢å¤–ä¿å­˜ä¸€ä»½ç‹¬ç«‹æ—¥å¿—ï¼Œå¯å–æ¶ˆä¸‹é¢æ³¨é‡Šï¼ˆè·¯å¾„éœ€ä¿®æ”¹ï¼‰
            # logging.FileHandler("python_internal.log", encoding="utf-8")
        ]
    )

    set_seed(SEED)
    model = get_model(MODEL_NAME, DEVICE, args=None)
    dataset = get_dataset(DATA_NAME, fast_split=False)

    used_keys, space, baseline = get_search_space()
    combos = enumerate_combinations(space, max_trials=MAX_TRIALS, seed=SEED, ensure_include=baseline)

    print(f"æœç´¢ç»„åˆæ•°ï¼š{len(combos)}ï¼ˆå…¨ç©ºé—´={np.prod([len(v) for v in space.values()])}ï¼‰")
    print(f"æœç´¢ç»´åº¦ï¼š{used_keys}")

    rows = []
    for i, cfg in enumerate(combos, 1):
        print(f'cfg:{cfg}')
        t0 = time.time()
        res = evaluate_combo(model, dataset, cfg, split="val", batch_size=EVAL_BATCH_SIZE)
        elapsed = time.time() - t0
        row = {
            "trial": i,
            "config": json.dumps(cfg, default=lambda o: o.item() if isinstance(o, np.generic) else str(o)),
            "mse": res["mse"], "mae": res["mae"], "smape": res["smape"],
            "time_sec": round(elapsed, 2),
        }
        rows.append(row)
        print(f"[val][{i}/{len(combos)}] mse={row['mse']:.6f} mae={row['mae']:.6f} smape={row['smape']:.3f} ({elapsed:.1f}s) cfg={cfg}")

    df_val = pd.DataFrame(rows).sort_values(PRIMARY_METRIC, ascending=True)
    df_val.to_csv(RESULT_CSV, index=False, encoding="utf-8")

    best_row = df_val.iloc[0]
    best_cfg = json.loads(best_row["config"])
    logging.info(f"éªŒè¯é›†ä¸Šæœ€ä¼˜ç»„åˆï¼ˆæŒ‰ {PRIMARY_METRIC}ï¼‰ä¸ºï¼š{best_cfg}ï¼Œç»“æœï¼šmse={best_row['mse']:.6f} mae={best_row['mae']:.6f} smape={best_row['smape']:.3f}")
    print("\n=== éªŒè¯é›†æœ€ä¼˜ç»„åˆï¼ˆæŒ‰ MSEï¼‰ ===")
    print(best_cfg)
    print(df_val.head(5)[["mse","mae","smape","time_sec","config"]])
    logging.info(f"å®Œæ•´æœç´¢ç»“æœå·²ä¿å­˜: {RESULT_CSV}")
    
    print("\n=== æµ‹è¯•é›†å¯¹æ¯”ï¼šbaseline vs best ===")
    base_res = evaluate_combo(model, dataset, baseline, split="test", batch_size=EVAL_BATCH_SIZE)
    best_res = evaluate_combo(model, dataset, best_cfg, split="test", batch_size=EVAL_BATCH_SIZE)

    def fmt(d): return f"mse={d['mse']:.6f}, mae={d['mae']:.6f}, smape={d['smape']:.3f}"
    print(f"baseline : {fmt(base_res)}")
    print(f"best     : {fmt(best_res)}")
    improve = (base_res["mse"] - best_res["mse"]) / max(1e-12, base_res["mse"]) * 100.0
    print(f"\nMSE æå‡ï¼š{improve:.2f}%  ï¼ˆæ­£æ•°=æ›´å¥½ï¼‰")
    logging.info(f"\nMSE æå‡ï¼š{improve:.2f}%  ï¼ˆæ­£æ•°=æ›´å¥½ï¼‰")
    print(f"\nå®Œæ•´æœç´¢ç»“æœå·²ä¿å­˜: {RESULT_CSV}")

if __name__ == "__main__":
    main()
